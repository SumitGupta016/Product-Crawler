Product Crawler
A scalable, domain-aware product crawler built with NestJS, Puppeteer, and Cheerio. It extracts product links from supported e-commerce websites, handling dynamic content and infinite scrolling effectively.â€‹

ğŸ§­ Supported Domains
Virgio

TataCliq

Nykaa Fashion

Westside

âš™ï¸ Features
Domain-Specific Configurations: Each domain has tailored configurations, including base URLs, product URL patterns, and optional page handling functions.

Dynamic Content Handling: Utilizes Puppeteer to handle JavaScript-rendered content and infinite scrolling.

Fallback Mechanism: If Puppeteer fails to extract links, falls back to Cheerio for static HTML parsing.

Modular Architecture: Built with NestJS for a clean and maintainable codebase.â€‹

ğŸ› ï¸ Installation
Clone the repository:

bash
Copy
Edit
git clone https://github.com/SumitGupta016/Product-Crawler.git
cd Product-Crawler
Install dependencies:

bash
Copy
Edit
npm install
ğŸš€ Running the Application
Development mode:

bash
Copy
Edit
npm run start:dev
Production mode:

bash
Copy
Edit
npm run start:prod
ğŸ§ª Testing
Run tests:

bash
Copy
Edit
npm run test
ğŸ“ Project Structure
bash
Copy
Edit
src/
â”œâ”€â”€ crawler/
â”‚ â”œâ”€â”€ crawler.service.ts # Core crawling logic
â”‚ â”œâ”€â”€ domains/
â”‚ â”‚ â””â”€â”€ crawler.domain.ts # Domain configurations
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ autoScroll.ts # Utility for auto-scrolling pages
â”œâ”€â”€ app.module.ts # Root module
â”œâ”€â”€ main.ts # Entry point
ğŸ“¦ Dependencies
@nestjs/common: Provides NestJS decorators and utilities.

cheerio: Fast, flexible, and lean implementation of core jQuery designed specifically for the server. Used for static HTML parsing.

puppeteer: Headless Chrome Node.js API. Used for rendering dynamic content and handling JavaScript-heavy pages.â€‹

ğŸ§© Domain Configuration
Each domain is configured with a DomainConfig interface:â€‹

typescript
Copy
Edit
export interface DomainConfig {
baseUrl: string;
productUrlPattern: RegExp;
handlePage?: (page: puppeteer.Page) => Promise<void>;
}
This allows for domain-specific handling, such as waiting for certain selectors or performing auto-scrolling.â€‹

ğŸ”§ AutoScroll Utility
The autoScroll function is used to simulate user scrolling on pages that load content dynamically as the user scrolls down. It's located in src/crawler/utils/autoScroll.ts.â€‹

typescript
Copy
Edit
import puppeteer from 'puppeteer';

export const autoScroll = async (page: puppeteer.Page): Promise<void> => {
await page.evaluate(async () => {
await new Promise<void>((resolve) => {
let totalHeight = 0;
const distance = 500;
const timer = setInterval(() => {
const scrollHeight = document.body.scrollHeight;
window.scrollBy(0, distance);
totalHeight += distance;

        if (totalHeight >= scrollHeight) {
          clearInterval(timer);
          resolve();
        }
      }, 300);
    });

});
};
ğŸ§  Application Flow
Input URL: The user provides a URL to crawl.

Domain Matching: The application checks if the URL matches any of the supported domains.

Puppeteer Crawling: If matched, Puppeteer is used to navigate to the page, perform any domain-specific handling (like scrolling), and extract product links.

Fallback to Cheerio: If Puppeteer fails to extract links, Cheerio is used to parse the static HTML and extract links.

Return Results: The extracted product links are returned.â€‹

ğŸ“„ License
This project is licensed under the MIT License. See the LICENSE file for details.â€‹
Medium

ğŸ¤ Contributing
Contributions are welcome! Please fork the repository and submit a pull request.â€‹
Medium
